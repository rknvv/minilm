train:
  out_dir: "./out"
  dataset_dir: "./data/pretrain"
  resume_from_checkpoint: false
  eval_interval: 100
  log_interval: 10
  eval_iters: 100
  always_save_checkpoint: true
  wandb_log: true
  wandb_project: "minilm"
  wandb_run_name: "run_20250408"
  gradient_accumulation_steps: 8
  train_batch_size: 256
  eval_batch_size: 256
  context_length: 512
  num_workers: 0
  learning_rate: 0.0006
  max_iters: 50000
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: true
  warmup_iters: 2000
  lr_decay_iters: 600000
  device: "cuda"
  dtype: "float16"
  compile: false
  backend: "nccl"
  eval_only: false

model:
  dim: 512
  n_layers: 16
  n_heads: 8
  n_kv_heads: 8
  vocab_size: 8192
  multiple_of: 256
  ffn_dim_multiplier: null
  norm_eps: 0.00001
  max_batch_size: 256
  max_seq_len: 512
  flash_attn: true
  dropout: 0.1